{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "# load a pre-trained english language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import nltk\n",
    "#nltk.download('opinion_lexicon')\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse the XML file\n",
    "tree = ET.parse('Restaurants.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Create empty lists to store data\n",
    "sentence_ids = []\n",
    "texts = []\n",
    "aspect_terms = []\n",
    "aspect_pos0 = []\n",
    "aspect_neg0 = []\n",
    "aspect_nue0 = []\n",
    "\n",
    "# Extract data from XML and populate the lists\n",
    "for sentence in root.findall('sentence'):\n",
    "    sentence_id = sentence.get('id')\n",
    "    text = sentence.find('text').text\n",
    "\n",
    "    aspect_pos = []\n",
    "    aspect_neg = []\n",
    "    aspect_nue = []\n",
    "    \n",
    "    aspect_terms_elem = sentence.find('aspectTerms')\n",
    "    if aspect_terms_elem is not None:\n",
    "        for aspect_term in aspect_terms_elem.findall('aspectTerm'):\n",
    "            term = aspect_term.get('term')\n",
    "            polarity = aspect_term.get('polarity')\n",
    "            from_index = int(aspect_term.get('from'))\n",
    "            to_index = int(aspect_term.get('to'))\n",
    "            if polarity == 'positive':\n",
    "                aspect_pos.append((from_index,to_index))\n",
    "            elif polarity == 'negative':\n",
    "                aspect_neg.append((from_index,to_index))\n",
    "            else:\n",
    "                aspect_nue.append((from_index,to_index))\n",
    "            #aspect_term_dict[(from_index,to_index)] = (polarity)\n",
    "    \n",
    "    sentence_ids.append(sentence_id)\n",
    "    texts.append(text)\n",
    "    aspect_pos0.append(aspect_pos)\n",
    "    aspect_neg0.append(aspect_neg)\n",
    "    aspect_nue0.append(aspect_nue)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "data = {\n",
    "    'Sentence ID': sentence_ids,\n",
    "    'Text': texts,\n",
    "    'Aspect_pos': aspect_pos0,\n",
    "    'Aspect_neg': aspect_neg0,\n",
    "    'Aspect_nue': aspect_nue0,\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "#Set the \"Sentence ID\" column as the index\n",
    "df.set_index('Sentence ID', inplace=True)\n",
    "\n",
    "# remove rows that Aspect_pos, Aspect_neg, Aspect_nue are all empty\n",
    "df = df[(df['Aspect_pos'].str.len() > 0) | (df['Aspect_neg'].str.len() > 0) | (df['Aspect_nue'].str.len() > 0)]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comvert aspect terms into one token if it is more than one token\n",
    "def convert_aspect_terms(from_index, to_index, text):\n",
    "    # extract the aspect term\n",
    "    aspect_term = text[from_index:to_index]\n",
    "    # if the aspect term is more than one token, convert it into one token\n",
    "    aspect_term = aspect_term.replace(' ','_')\n",
    "    text = text[:from_index] + aspect_term + text[to_index:]\n",
    "    return text\n",
    "\n",
    "# replce '-' with '_' in all text\n",
    "for index, row in df.iterrows():\n",
    "    row['Text'] = row['Text'].replace('-','_')\n",
    "# # convert aspect terms into one token if it is more than one token\n",
    "for index, row in df.iterrows():\n",
    "    index_list = row['Aspect_pos'] + row['Aspect_neg'] + row['Aspect_nue']\n",
    "    for from_index, to_index in index_list:\n",
    "        row['Text'] = convert_aspect_terms(from_index, to_index, row['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Aspect_pos</th>\n",
       "      <th>Aspect_neg</th>\n",
       "      <th>Aspect_nue</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3121</th>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(8, 13)]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2777</th>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>[(57, 61)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>[(4, 8), (55, 62)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(141, 145)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2846</th>\n",
       "      <td>Not only was the food outstanding, but the lit...</td>\n",
       "      <td>[(17, 21), (51, 56)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>Our agreed favorite is the orrechiete_with_sau...</td>\n",
       "      <td>[(27, 62), (76, 83)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(152, 157), (113, 117)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          Text  \\\n",
       "Sentence ID                                                      \n",
       "3121                      But the staff was so horrible to us.   \n",
       "2777         To be completely fair, the only redeeming fact...   \n",
       "1634         The food is uniformly exceptional, with a very...   \n",
       "2846         Not only was the food outstanding, but the lit...   \n",
       "1458         Our agreed favorite is the orrechiete_with_sau...   \n",
       "\n",
       "                       Aspect_pos Aspect_neg                Aspect_nue  \n",
       "Sentence ID                                                             \n",
       "3121                           []  [(8, 13)]                        []  \n",
       "2777                   [(57, 61)]         []                        []  \n",
       "1634           [(4, 8), (55, 62)]         []              [(141, 145)]  \n",
       "2846         [(17, 21), (51, 56)]         []                        []  \n",
       "1458         [(27, 62), (76, 83)]         []  [(152, 157), (113, 117)]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Aspect_pos:  2164\n",
      "Total number of Aspect_neg:  805\n",
      "Total number of Aspect_nue:  724\n"
     ]
    }
   ],
   "source": [
    "# count how many Aspect_pos in total\n",
    "count_pos = 0\n",
    "count_neg = 0\n",
    "count_nue = 0\n",
    "for index, row in df.iterrows():\n",
    "    count_pos += len(row['Aspect_pos'])\n",
    "    count_neg += len(row['Aspect_neg'])\n",
    "    count_nue += len(row['Aspect_nue'])\n",
    "\n",
    "print('Total number of Aspect_pos: ', count_pos)\n",
    "print('Total number of Aspect_neg: ', count_neg)\n",
    "print('Total number of Aspect_nue: ', count_nue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text by the sentence id\n",
    "def extract_text_by_id(sentence_id):\n",
    "    text = nlp(df.loc[sentence_id]['Text'])\n",
    "    text = str(text)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def into_parsed_table(text):\n",
    "    doc = nlp(text)\n",
    "    # get dependencies\n",
    "    parsed = pd.DataFrame(columns=[\"token\", \"dep\", \"head\", \"head_pos\", \"children\"])\n",
    "    for token in doc:\n",
    "        row = pd.DataFrame([{\n",
    "            \"token\": token.text,\n",
    "            \"dep\": token.dep_,\n",
    "            \"head\": token.head.text,\n",
    "            \"head_pos\": token.head.pos_,\n",
    "            \"children\": str([f\"{child}\" for child in token.children])\n",
    "        }])\n",
    "        parsed = pd.concat([parsed, row], axis=0)\n",
    "    return parsed\n",
    "\n",
    "def into_parsed_pic(text):\n",
    "    doc = nlp(text)\n",
    "    displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after compare with vader_lexicon, opinion_lexicon is better\n",
    "\n",
    "# import nltk\n",
    "# #nltk.download('vader_lexicon')\n",
    "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# def is_neutral(word):\n",
    "#     sentiment_scores = sia.polarity_scores(word)\n",
    "#     del sentiment_scores['compound']\n",
    "#     return max(sentiment_scores, key=lambda key: sentiment_scores[key]) == 'neu'\n",
    "\n",
    "# def is_positive(word):\n",
    "#     sentiment_scores = sia.polarity_scores(word)\n",
    "#     del sentiment_scores['compound']\n",
    "#     return max(sentiment_scores, key=lambda key: sentiment_scores[key]) == 'pos'\n",
    "\n",
    "# def is_negative(word):\n",
    "#     sentiment_scores = sia.polarity_scores(word)\n",
    "#     del sentiment_scores['compound']\n",
    "#     return max(sentiment_scores, key=lambda key: sentiment_scores[key]) == 'neg'\n",
    "\n",
    "# is_negative('yucky')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build postive and negative word list\n",
    "positive_words = opinion_lexicon.positive()\n",
    "positive_words = list(positive_words)\n",
    "adding_pos_words = [\"yummy\",\"favorite\",\"agreed\",\"expert\",\"chilled\",\"unheralded\",\"impecible\",\"magnificant\",\"tasty\",\"flavorful\",\"try\"]\n",
    "positive_words.extend(adding_pos_words)\n",
    "\n",
    "negative_words = opinion_lexicon.negative()\n",
    "negative_words = list(negative_words)\n",
    "adding_neg_words = [\"yucky\",\"tasteless\",\"bland\",\"gross\",\"unappetizing\",\"subpar\",\"plain\"]\n",
    "negative_words.extend(adding_neg_words)\n",
    "\n",
    "nuetral_words = [\"average\",\"tipically\",\"ordinary\",\"standard\",\"usual\",\"typical\",\"unremarkable\",\"regular\",\"common\",\"so-so\",\"passable\",\"mediocre\",\"moderate\",\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has_neg_dep helper function\n",
    "# if aspect_term's sibling/head/children contains 'neg' dependency >>  return True\n",
    "def has_neg_dep(text, aspect_term):\n",
    "    doc = nlp(text)\n",
    "    has_neg = False\n",
    "    for token in doc:\n",
    "        if token.text.lower() == aspect_term.lower():\n",
    "            for sibling in token.head.children:\n",
    "                if sibling.dep_ == 'neg':\n",
    "                    has_neg = True\n",
    "                # for subchild in sibling.children:\n",
    "                #     if subchild.dep_ == 'neg':\n",
    "                #         has_neg = True\n",
    "            for child in token.children:\n",
    "                if child.dep_ == 'neg':\n",
    "                    has_neg = True\n",
    "            if token.head.dep_ == 'neg':\n",
    "                has_neg = True\n",
    "            # if token.head.head.dep_ == 'neg':\n",
    "            #     has_neg = True\n",
    "\n",
    "        if has_neg:\n",
    "            return True  \n",
    "    return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive rule1\n",
    "# aspect_term's children/subchildren or ancestor are in positive words\n",
    "def positive_rule1(text, aspect_term, has_neg):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() == aspect_term.lower():\n",
    "            for child in token.children:\n",
    "                if (child.text.lower()) in positive_words and not has_neg:\n",
    "                    return True\n",
    "                for subchild in child.children:\n",
    "                    if subchild.dep_ in ['amod','advmod','acomp','attr','nsubj'] and (subchild.text.lower()) in positive_words and not has_neg:\n",
    "                        return True\n",
    "            if token.head.text.lower() in positive_words:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive rule2\n",
    "# aspect_term's siblings with certain dependency are in positive words\n",
    "def positive_rule2(text, aspect_term, has_neg):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() == aspect_term.lower():\n",
    "            for child in token.head.children:\n",
    "                if child.dep_ in ['amod','advmod','acomp','attr','nsubj','conj'] and (child.text.lower())in positive_words and not has_neg:\n",
    "                    return True\n",
    "                for subchild in child.children:\n",
    "                    if subchild.dep_ in ['amod','advmod','acomp','attr','nsubj','conj'] and (subchild.text.lower()) in positive_words and not has_neg:\n",
    "                        return True\n",
    "                  \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive rule3\n",
    "# if aspect_term's ancestor is in certain dependency, keep going up until the ancestor is not in that dependency\n",
    "# and check that ancestor's children\n",
    "def positive_rule3(text, aspect_term, has_neg):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() == aspect_term.lower():\n",
    "            node = token.head\n",
    "            while node.dep_ in ['conj','compound','prep','pobj']:\n",
    "                node = node.head\n",
    "            if node.text.lower() in positive_words and not has_neg:\n",
    "                return True\n",
    "            for child in node.children:\n",
    "                if (child.text.lower()) in positive_words and not has_neg:\n",
    "                    return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# think about negation case, but \"not bad\" is not very positive for my understanding\n",
    "# tried to add, but not very useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all done, total pos terms are 2164\n"
     ]
    }
   ],
   "source": [
    "# evaluate the positive rules\n",
    "def pos_evaluation():\n",
    "    outcome =[]\n",
    "    count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        all_aspects = row['Aspect_pos'] + row['Aspect_neg'] + row['Aspect_nue']\n",
    "        for from_index, to_index in all_aspects:  \n",
    "            if (from_index, to_index) in row['Aspect_pos']:\n",
    "                ture_label = \"true\"#\"positive\"\n",
    "            else:\n",
    "                ture_label = \"others\"\n",
    "\n",
    "            aspect_term = row['Text'][from_index:to_index]\n",
    "            has_neg = has_neg_dep(row['Text'], aspect_term)\n",
    "\n",
    "            pred1 = positive_rule1(row['Text'], aspect_term, has_neg)\n",
    "            pred2 = positive_rule2(row['Text'], aspect_term, has_neg)\n",
    "            pred3 = positive_rule3(row['Text'], aspect_term, has_neg)\n",
    "            \n",
    "\n",
    "            if pred2 == True or pred3 == True or pred1 == True:\n",
    "                pred_label = \"true\"#\"positive\"\n",
    "            else:\n",
    "                pred_label = \"others\"\n",
    "\n",
    "            outcome.append((ture_label, pred_label))\n",
    "                \n",
    "    print(\"all done, total pos terms are 2164\")\n",
    "    return outcome\n",
    "\n",
    "pos_outcome = pos_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1185  344]\n",
      " [ 680 1484]]\n"
     ]
    }
   ],
   "source": [
    "# Extract the ground truth and predictions into separate lists\n",
    "ground_truth = [item[0] for item in pos_outcome]\n",
    "predictions = [item[1] for item in pos_outcome]\n",
    "\n",
    "# Create the confusion matrix\n",
    "confusion_mat = confusion_matrix(ground_truth, predictions)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Recall: 0.6857670979667283\n",
      "Positive Precision: 0.811816192560175\n",
      "Positive F1 score: 0.7434869739478958\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision, recall and F1 score\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(ground_truth, predictions).ravel()\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Positive Recall:\", recall)# 在所有的positive里面，有多少能被判断出来\n",
    "print(\"Positive Precision:\", precision) #在判断为positive的里面，有多少是对的\n",
    "print(\"Positive F1 score:\", f1_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative rule1\n",
    "# aspect_term's children/subchildren or ancestor are in negative words\n",
    "def negative_rule1(text, aspect_term):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() == aspect_term.lower():\n",
    "            for child in token.children:\n",
    "                if (child.text.lower()) in negative_words:\n",
    "                    return True\n",
    "                \n",
    "                for subchild in child.children:\n",
    "                    if subchild.dep_ in ['amod','advmod','acomp','attr','nsubj'] and (subchild.text.lower()) in negative_words:\n",
    "                        return True\n",
    "                    \n",
    "         \n",
    "            if (token.head.text.lower()) in negative_words:\n",
    "                return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative rule2\n",
    "# aspect_term's siblings with certain dependency are in negative words\n",
    "def negative_rule2(text, aspect_term):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() == aspect_term.lower():\n",
    "            for child in token.head.children:\n",
    "                if child.dep_ in ['amod','advmod','acomp','attr','nsubj'] and (child.text.lower()) in negative_words:\n",
    "                    return True\n",
    "              \n",
    "                for subchild in child.children:\n",
    "                    if subchild.dep_ in ['amod','advmod','acomp','attr','nsubj','conj'] and (subchild.text.lower()) in negative_words:\n",
    "                        return True\n",
    "                  \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative rule3\n",
    "# if aspect_term's ancestor is in certain dependency, keep going up until the ancestor is not in that dependency\n",
    "# and check that ancestor's children\n",
    "def negative_rule3(text, aspect_term):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() == aspect_term.lower():\n",
    "            node = token.head\n",
    "            while node.dep_ in ['conj','compound','prep','pobj']:\n",
    "                node = node.head\n",
    "            if node.text.lower() in negative_words:\n",
    "                return True\n",
    "           \n",
    "            \n",
    "            for child in node.children:\n",
    "                if (child.text.lower()) in negative_words:\n",
    "                    return True\n",
    "                \n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all done, total neg terms are 805\n"
     ]
    }
   ],
   "source": [
    "# check the next sentence that doesn't fit the rules\n",
    "def neg_evaluation():\n",
    "    outcome =[]\n",
    "    for index, row in df.iterrows():\n",
    "        all_aspects = row['Aspect_pos'] + row['Aspect_neg'] + row['Aspect_nue']\n",
    "        for from_index, to_index in all_aspects:  \n",
    "            if (from_index, to_index) in row['Aspect_neg']:\n",
    "                ture_label = \"true\"#\"negative\"\n",
    "            else:\n",
    "                ture_label = \"others\"\n",
    "\n",
    "            aspect_term = row['Text'][from_index:to_index]\n",
    "\n",
    "            pred1 = negative_rule1(row['Text'], aspect_term)\n",
    "            pred2 = negative_rule2(row['Text'], aspect_term)\n",
    "            pred3 = negative_rule3(row['Text'], aspect_term)\n",
    "            pred4 = has_neg_dep(row['Text'], aspect_term)\n",
    "            \n",
    "\n",
    "            if pred2 == True or pred1 == True or pred3 == True or pred4 == True:\n",
    "                pred_label = \"true\"#\"negative\"\n",
    "            else:\n",
    "                pred_label = \"others\"\n",
    "\n",
    "            outcome.append((ture_label, pred_label))\n",
    "                \n",
    "    print(\"all done, total neg terms are 805\")\n",
    "    return outcome\n",
    "\n",
    "neg_outcome = neg_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2625  263]\n",
      " [ 479  326]]\n"
     ]
    }
   ],
   "source": [
    "# Extract the ground truth and predictions into separate lists\n",
    "ground_truth = [item[0] for item in neg_outcome]\n",
    "predictions = [item[1] for item in neg_outcome]\n",
    "\n",
    "# Create the confusion matrix\n",
    "confusion_mat = confusion_matrix(ground_truth, predictions)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Recall: 0.4049689440993789\n",
      "Negative Precision: 0.5534804753820034\n",
      "Negative F1 score: 0.46771879483500717\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision, recall and F1 score\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(ground_truth, predictions).ravel()\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "print(\"Negative Recall:\", recall) # 在所有的negative里面，有多少能被判断出来\n",
    "print(\"Negative Precision:\", precision) # 在判断为negative的里面，有多少是对的\n",
    "print(\"Negative F1 score:\", f1_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neutral rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuetral rule0\n",
    "# aspect_term is either positive or negative, but not both >> neutral\n",
    "def neutral_rule0(text, aspect_term, has_neg):\n",
    "    is_pos = (positive_rule1(text, aspect_term, has_neg) or positive_rule2(text, aspect_term, has_neg) or positive_rule3(text, aspect_term, has_neg))\n",
    "    is_neg = (negative_rule1(text, aspect_term) or negative_rule2(text, aspect_term) or negative_rule3(text, aspect_term) or has_neg)\n",
    "    if is_pos and is_neg:\n",
    "        return True\n",
    "    if is_pos or is_neg:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative rule1\n",
    "# aspect_term's children/subchildren or ancestor are in neutral words\n",
    "def neutral_rule1(text, aspect_term):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() == aspect_term.lower():\n",
    "            for child in token.children:\n",
    "                if (child.text.lower()) in nuetral_words:\n",
    "                    return True\n",
    "                \n",
    "                for subchild in child.children:\n",
    "                    if subchild.dep_ in ['amod','advmod','acomp','attr','nsubj'] and (subchild.text.lower()) in nuetral_words:\n",
    "                        return True\n",
    "         \n",
    "            if (token.head.text.lower()) in nuetral_words:\n",
    "                return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutral rule2\n",
    "# aspect_term's siblings with certain dependency are in neutral words\n",
    "def neutral_rule2(text, aspect_term):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() == aspect_term.lower():\n",
    "            for child in token.head.children:\n",
    "                if child.dep_ in ['amod','advmod','acomp','attr','nsubj'] and (child.text.lower()) in nuetral_words:\n",
    "                    return True\n",
    "                for subchild in child.children:\n",
    "                    if subchild.dep_ in ['amod','advmod','acomp','attr','nsubj','conj'] and (subchild.text.lower()) in nuetral_words:\n",
    "                        return True\n",
    "                  \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutral rule3\n",
    "# if aspect_term's ancestor is in certain dependency, keep going up until the ancestor is not in that dependency\n",
    "# and check that ancestor's children\n",
    "def neutral_rule3(text, aspect_term):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.lower() == aspect_term.lower():\n",
    "            node = token.head\n",
    "            while node.dep_ in ['conj','compound','prep','pobj']:\n",
    "                node = node.head\n",
    "            if node.text.lower() in nuetral_words:\n",
    "                return True\n",
    "\n",
    "            for child in node.children:\n",
    "                if (child.text.lower()) in nuetral_words:\n",
    "                    return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all done, total pos terms are 724\n"
     ]
    }
   ],
   "source": [
    "# evaluate neutral terms\n",
    "def neu_evaluation():\n",
    "    outcome =[]\n",
    "    count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        all_aspects = row['Aspect_pos'] + row['Aspect_neg'] + row['Aspect_nue']\n",
    "        for from_index, to_index in all_aspects:  \n",
    "            if (from_index, to_index) in row['Aspect_nue']:\n",
    "                ture_label = \"true\"#\"neutral\"\n",
    "            else:\n",
    "                ture_label = \"others\"\n",
    "\n",
    "            aspect_term = row['Text'][from_index:to_index]\n",
    "            has_neg = has_neg_dep(row['Text'], aspect_term)\n",
    "\n",
    "            pred1 = neutral_rule1(row['Text'], aspect_term)\n",
    "            pred2 = neutral_rule2(row['Text'], aspect_term)\n",
    "            pred3 = neutral_rule3(row['Text'], aspect_term)\n",
    "            pred0 = neutral_rule0(row['Text'], aspect_term, has_neg)\n",
    "\n",
    "\n",
    "            if pred2 == True or pred1 == True or pred3 == True or pred0 == True: \n",
    "                pred_label = \"true\"#\"neutral\"\n",
    "            else:\n",
    "                pred_label = \"others\"\n",
    "\n",
    "            outcome.append((ture_label, pred_label))\n",
    "                \n",
    "    print(\"all done, total pos terms are 724\")\n",
    "    return outcome\n",
    "\n",
    "neu_outcome = neu_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1861 1108]\n",
      " [ 224  500]]\n"
     ]
    }
   ],
   "source": [
    "# Extract the ground truth and predictions into separate lists\n",
    "ground_truth = [item[0] for item in neu_outcome]\n",
    "predictions = [item[1] for item in neu_outcome]\n",
    "\n",
    "\n",
    "# Create the confusion matrix\n",
    "confusion_mat = confusion_matrix(ground_truth, predictions)\n",
    "\n",
    "print(confusion_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuetral Recall: 0.6906077348066298\n",
      "Nuetral Precision: 0.31094527363184077\n",
      "Nuetral F1 score: 0.42881646655231553\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tn, fp, fn, tp = confusion_matrix(ground_truth, predictions).ravel()\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "print(\"Nuetral Recall:\", recall)# 在所有的nuetral里面，有多少能被判断出来 \n",
    "print(\"Nuetral Precision:\", precision)#在判断为neutral的里面，有多少是对的\n",
    "print(\"Nuetral F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
